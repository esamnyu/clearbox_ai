{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c88f4bb",
   "metadata": {},
   "source": [
    "## Steering Vectors: Activation Engineering in GPT-2\n",
    "\n",
    "This notebook implements **Activation Steering**, a technique to control model behavior and prediction by intervening in the residual stream during inference. A good reference I came across for this is [here](https://www.lesswrong.com/posts/ndyngghzFY388Dnew/implementing-activation-steering).\n",
    "\n",
    "**Core Concept:**\n",
    "$$ h_{steered} = h_{original} + \\alpha \\cdot \\vec{v}_{concept} $$\n",
    "\n",
    "1. **Extract** a \"sentiment direction\" using contrastive prompt pairs\n",
    "2. **Visualize** this direction using PCA (building our previous work)\n",
    "3. **Intervene** with Pytorch hooks to steer generation towards positive or negative sentiment\n",
    "4. **Validate** the effect quantitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9520cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the model load stuff\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
