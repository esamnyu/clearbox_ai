{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91eec16",
   "metadata": {},
   "source": [
    "# Interpretability Tests and Experiments\n",
    "\n",
    "Going to do some prelim tests to show case mech interp techniques, mathematical unpinnings of said techniques and how it applies to the larger project.\n",
    "\n",
    "This notebook serves as the proof-of-concept for the \"Researcher Layer\" of Clear-box AI web app. \n",
    "It validates the mathematical operations required for:\n",
    "1. **Signal Extraction:** Capturing hidden states and attentions.\n",
    "2. **Logit Attribution:** Understanding next-token prediction via the Logit Lens.\n",
    "3. **Concept Visualization:** PCA projection of the residual stream.\n",
    "\n",
    "**Model:** GPT-2 (Small)\n",
    "**Goal:** Validate logic before porting to TypeScript/WebGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fcc347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Mahmoud Shabana/Documents/software_projects/clearbox_ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3554b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc23ef7",
   "metadata": {},
   "source": [
    "### Engine Layer:\n",
    "\n",
    "This corresponds to the `src/engine/ModelManager.ts`.\n",
    "We use standard HuggingFace, but we explicitly request `output_hidden_states` and `output_attentions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9327d6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15748053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(prompt: str):\n",
    "    \"\"\"\n",
    "    Run inference and capture internals.\n",
    "    Equivalent to the Web Worker's `generate` func with full observability.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **inputs,\n",
    "            output_hidden_states=True, # ref for this on HF: https://huggingface.co/docs/transformers/v4.26.0/main_classes/output#transformers.modeling_outputs.BaseModelOutput.hidden_states\n",
    "            output_attentions=True\n",
    "        )\n",
    "    \n",
    "    # hidden_states: Tuple of (layer_count + 1) tensors. Shape: [batch, seq, hidden]\n",
    "    # attentions: Tuple of (layer_count) tensors. Shape: [batch, heads, seq, seq]\n",
    "    # logits: Shape [batch, seq, vocab]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"tokens\": [tokenizer.decode([t]) for t in inputs[\"input_ids\"][0]],\n",
    "        \"hidden_states\": outputs.hidden_states,\n",
    "        \"attentions\": outputs.attentions,\n",
    "        \"logits\": outputs.logits\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06821b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: ['The', ' E', 'iff', 'el', ' Tower', ' is', ' located', ' in', ' the', ' city', ' of']\n",
      "Total layers captured: 13\n"
     ]
    }
   ],
   "source": [
    "# quick test run of inference func\n",
    "prompt = \"The Eiffel Tower is located in the city of\"\n",
    "data = run_inference(prompt)\n",
    "print(f\"Prompt tokens: {data['tokens']}\")\n",
    "print(f\"Total layers captured: {len(data['hidden_states'])}\") # should be 13 => Embed + 12 blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b718ea",
   "metadata": {},
   "source": [
    "## Logit Attribution\n",
    "\n",
    "This technique applies the Unembedding Matrix ($W_U$) to the hidden state of *intermediate* layers. It answers: \"If we stopped the model at Layer $L$, what token would it predict?\"\n",
    "\n",
    "Mathematical Logic: $ Logits_L = h_L \\cdot W_U^T $\n",
    "\n",
    "This validates the logic for `src/vis/LogitDistribution.tsx`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
