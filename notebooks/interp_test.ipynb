{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91eec16",
   "metadata": {},
   "source": [
    "# Interpretability Tests and Experiments\n",
    "\n",
    "Going to do some prelim tests to show case mech interp techniques, mathematical unpinnings of said techniques and how it applies to the larger project.\n",
    "\n",
    "This notebook serves as the proof-of-concept for the \"Researcher Layer\" of Clear-box AI web app. \n",
    "It validates the mathematical operations required for:\n",
    "1. **Signal Extraction:** Capturing hidden states and attentions.\n",
    "2. **Logit Attribution:** Understanding next-token prediction via the Logit Lens.\n",
    "3. **Concept Visualization:** PCA projection of the residual stream.\n",
    "\n",
    "**Model:** GPT-2 (Small)\n",
    "**Goal:** Validate logic before porting to TypeScript/WebGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fcc347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Mahmoud Shabana/Documents/software_projects/clearbox_ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3554b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc23ef7",
   "metadata": {},
   "source": [
    "### Engine Layer:\n",
    "\n",
    "This corresponds to the `src/engine/ModelManager.ts`.\n",
    "We use standard HuggingFace, but we explicitly request `output_hidden_states` and `output_attentions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9327d6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15748053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(prompt: str):\n",
    "    \"\"\"\n",
    "    Run inference and capture internals.\n",
    "    Equivalent to the Web Worker's `generate` func with full observability.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **inputs,\n",
    "            output_hidden_states=True, # ref for this on HF: https://huggingface.co/docs/transformers/v4.26.0/main_classes/output#transformers.modeling_outputs.BaseModelOutput.hidden_states\n",
    "            output_attentions=True\n",
    "        )\n",
    "    \n",
    "    # hidden_states: Tuple of (layer_count + 1) tensors. Shape: [batch, seq, hidden]\n",
    "    # attentions: Tuple of (layer_count) tensors. Shape: [batch, heads, seq, seq]\n",
    "    # logits: Shape [batch, seq, vocab]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"tokens\": [tokenizer.decode([t]) for t in inputs[\"input_ids\"][0]],\n",
    "        \"hidden_states\": outputs.hidden_states,\n",
    "        \"attentions\": outputs.attentions,\n",
    "        \"logits\": outputs.logits\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06821b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: ['The', ' E', 'iff', 'el', ' Tower', ' is', ' located', ' in', ' the', ' city', ' of']\n",
      "Total layers captured: 13\n"
     ]
    }
   ],
   "source": [
    "# quick test run of inference func\n",
    "prompt = \"The Eiffel Tower is located in the city of\"\n",
    "data = run_inference(prompt)\n",
    "print(f\"Prompt tokens: {data['tokens']}\")\n",
    "print(f\"Total layers captured: {len(data['hidden_states'])}\") # should be 13 => Embed + 12 blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b718ea",
   "metadata": {},
   "source": [
    "## Logit Attribution\n",
    "\n",
    "This technique applies the Unembedding Matrix ($W_U$) to the hidden state of *intermediate* layers. It answers: \"If we stopped the model at Layer $L$, what token would it predict?\"\n",
    "\n",
    "Mathematical Logic: $ Logits_L = h_L \\cdot W_U^T $\n",
    "\n",
    "This validates the logic for `src/vis/LogitDistribution.tsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e6b7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens(hidden_states, model, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Apply language model head to every layer's hidden state.\n",
    "    \"\"\"\n",
    "    unembed_matrix = model.lm_head.weight # shape: [vocab, hidden]\n",
    "    layer_preds = []\n",
    "\n",
    "    for layer_idx, hidden in enumerate(hidden_states):\n",
    "        # the hidden shape = [1, seq_len, 768]\n",
    "        # only interested in the prediction at the last token pos\n",
    "        last_hidden_token = hidden[0, -1, :] # shape: [768]\n",
    "\n",
    "        # Project to vocab space\n",
    "        # In the standard GPT-2, LayerNorm(ln_f) is applied before the head.\n",
    "        # The strict logit lens usually bypasses ln_f for raw stream analysis\n",
    "        # but for prediction parity, we should ideally apply it at the final layer.\n",
    "        # We'll do a direct projection for raw residual stream analysis.\n",
    "        logits = torch.matmul(unembed_matrix, last_hidden_token)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, top_idxs = torch.topk(probs, top_k)\n",
    "\n",
    "        preds = []\n",
    "        for rank, (prob, idx) in enumerate(zip(top_probs, top_idxs)):\n",
    "            token = tokenizer.decode([idx.item()])\n",
    "            preds.append(f\"{token} ({prob:.2f})\")\n",
    "        layer_preds.append(preds)\n",
    "    \n",
    "    return layer_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7885c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target completion expected: ' Paris'\n",
      "\n",
      "Embed      | Top prediction:  of (0.00)\n",
      "Layer 0    | Top prediction:  the (1.00)\n",
      "Layer 1    | Top prediction:  the (1.00)\n",
      "Layer 2    | Top prediction:  the (1.00)\n",
      "Layer 3    | Top prediction:  the (1.00)\n",
      "Layer 4    | Top prediction:  the (1.00)\n",
      "Layer 5    | Top prediction:  the (1.00)\n",
      "Layer 6    | Top prediction:  the (1.00)\n",
      "Layer 7    | Top prediction:  the (1.00)\n",
      "Layer 8    | Top prediction:  the (1.00)\n",
      "Layer 9    | Top prediction:  the (1.00)\n",
      "Layer 10   | Top prediction:  the (1.00)\n",
      "Layer 11   | Top prediction:  Paris (0.06)\n"
     ]
    }
   ],
   "source": [
    "# execute logit lens\n",
    "predictions = logit_lens(data[\"hidden_states\"], model, tokenizer)\n",
    "\n",
    "print(f\"Target completion expected: ' Paris'\\n\")\n",
    "for i, preds in enumerate(predictions):\n",
    "    layer_name = \"Embed\" if i == 0 else f\"Layer {i-1}\"\n",
    "    print(f\"{layer_name: <10} | Top prediction: {preds[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
